{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3180e070-9959-41cb-b0c2-23cf029bc04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 18:18:20.618966: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735409900.638331   56937 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735409900.645155   56937 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-28 18:18:20.672613: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "from tensorflow.data import Dataset \n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras import Model\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143b6eee-4526-40a9-b074-c13be0739e14",
   "metadata": {},
   "source": [
    "#### Setup and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2227c9e-eb68-4dc6-848c-af93a20f6eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.json\") as file:\n",
    "    config = json.load(file)\n",
    " \n",
    "aptos2019_path = Path(config[\"dataset\"][\"aptos_2019\"][\"balanced_resized\"])\n",
    "models_trained_path = Path(config[\"models_trained\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd0c56b-461f-42d9-9244-a16650b9ba54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1735409903.438944   56937 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting sizes:\n",
      "- train size : 12959\n",
      "- val size : 3702\n",
      "- test size : 1851\n",
      "\n",
      "Example Tensor:\n",
      "Batched Input shape: (8, 224, 224, 3)\n",
      "Batched One-hot Tensor Shape: (8, 5)\n",
      "One-hot Tensor numpy [0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 18:18:24.442505: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "csv = pd.read_csv(aptos2019_path / \"labels.csv\")\n",
    "images_path = str(aptos2019_path / \"images\")\n",
    "\n",
    "num_class = 5\n",
    "def name2image_class2onehot(tensor):\n",
    "    # Loading and processing image\n",
    "    image_name = \"/\" + tensor[\"img_name\"] + \".png\"\n",
    "    image_path = images_path + image_name\n",
    "    image_file = tf.io.read_file(image_path)\n",
    "    image_file = tf.io.decode_png(image_file,channels=3)\n",
    "    image_file = tf.cast(image_file,tf.float32)\n",
    "    image_file = tf.image.resize(image_file,[224,224])\n",
    "    image_file /= 255.0\n",
    "    # onehotting the class \n",
    "    onehot_class = tf.one_hot(tensor[\"class\"], depth = num_class)\n",
    "    \n",
    "    return (image_file, onehot_class)\n",
    "\n",
    "tf.random.set_seed(342)\n",
    "aptos2019_dataset = (Dataset\n",
    "                     .from_tensor_slices(dict(csv))\n",
    "                     .shuffle(len(csv))\n",
    "                     .map(name2image_class2onehot))\n",
    "\n",
    "# Selecting sizes of train, val and test dataset\n",
    "dataset_size = dict() \n",
    "dataset_size[\"train\"] = 0.7 \n",
    "dataset_size[\"val\"]   = 0.2\n",
    "dataset_size[\"test\"]  = 0.1\n",
    "\n",
    "# creating train, test, val datasets\n",
    "for key in dataset_size:\n",
    "    dataset_size[key] = int(dataset_size[key] * len(aptos2019_dataset))\n",
    "dataset = dict()\n",
    "dataset[\"train\"] = aptos2019_dataset.take(dataset_size[\"train\"]) \n",
    "dataset[\"val\"] = aptos2019_dataset.skip(dataset_size[\"train\"]).take(dataset_size[\"val\"])\n",
    "dataset[\"test\"] = aptos2019_dataset.skip(dataset_size[\"train\"] + dataset_size[\"val\"]).take(dataset_size[\"test\"]) \n",
    "\n",
    "# batching the splits\n",
    "batch_size = 8\n",
    "dataset[\"train\"] = dataset[\"train\"].batch(batch_size)\n",
    "dataset[\"val\"] = dataset[\"val\"].batch(batch_size)\n",
    "#  dataset[\"test\"] = dataset[\"test\"].batch(batch_size)\n",
    "\n",
    "print(\"Splitting sizes:\")\n",
    "for key in dataset_size:\n",
    "    print(f\"- {key} size : {dataset_size[key]}\")\n",
    "\n",
    "# example of a tensor from train set\n",
    "print(\"\\nExample Tensor:\")\n",
    "for tensor in dataset[\"train\"].take(1):\n",
    "    print(\"Batched Input shape:\", tensor[0].shape)\n",
    "    print(\"Batched One-hot Tensor Shape:\", tensor[1].shape)\n",
    "    print(\"One-hot Tensor numpy\", tensor[1].numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e97b7cb-cdbf-4e4b-80cf-564e2e994e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam(input, output, model, last_conv_layer_name):\n",
    "    \n",
    "    # Creating grad_model for both activation and predictions\n",
    "    last_conv_layer = model.get_layer(last_conv_layer_name)\n",
    "    grad_mod = tf.keras.models.Model( \n",
    "        inputs = model.input, \n",
    "        outputs = [last_conv_layer.output, model.output] \n",
    "    )\n",
    "\n",
    "    # Wrap for watching with tape\n",
    "    x = tf.Variable(input) \n",
    "\n",
    "    # Forward pass \n",
    "    with tf.GradientTape() as tape:\n",
    "        activations, predictions = grad_mod(x) \n",
    "        class_channel = predictions[:,output] \n",
    "\n",
    "    # How much the activation affects the class channel\n",
    "    grads = tape.gradient(class_channel, activations)\n",
    "\n",
    "    # Average of all the kernel gradients\n",
    "    activation_weights = tf.reduce_mean(grads, axis=(0, 1, 2)) \n",
    "    # Alternative\n",
    "    # activation_weights = np.sum(grads, axis=(0,1,2)) / 196 #  1 * 14 * 14\n",
    "\n",
    "    activations = activations[0]\n",
    "\n",
    "    # Add all activation maps with computed weights\n",
    "    heatmap = np.dot(activations, activation_weights) \n",
    "    # Alternative\n",
    "    # heatmap = activations @ pooled_grads[..., tf.newaxis]\n",
    "    # heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # ReLU to remove negative activations\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    heatmap = heatmap.numpy()\n",
    "    \n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d3ac426-f012-491d-ae01-8cbbe34631bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_heatmap(image, heatmap):\n",
    "    alpha = 1\n",
    "    beta = 0.001\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    image_to_plot = heatmap\n",
    "    image_to_plot = cv2.resize(image_to_plot, image.shape[0:-1], interpolation = cv2.INTER_LINEAR)\n",
    "    image_to_plot = cv2.normalize(image_to_plot, None, 0, 255, cv2.NORM_MINMAX) \n",
    "    image_to_plot = cv2.applyColorMap(image_to_plot.astype(np.uint8),cv2.COLORMAP_JET)\n",
    "    image_to_plot = cv2.addWeighted(image, alpha, image_to_plot.astype(np.float32), beta, 0)\n",
    "    image_to_plot = cv2.normalize(image_to_plot,None, 0, 1, cv2.NORM_MINMAX)\n",
    "    image_to_plot = cv2.cvtColor(image_to_plot, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return image_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd3dfea7-6088-483f-8d7d-de9beffc28a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grad_cam_plots(model,last_conv_layer_name,data, num_images = 10):\n",
    "    for x in data.batch(1).take(num_images):\n",
    "        \n",
    "        image = x[0].numpy()[0] \n",
    "        target = np.argmax(x[1].numpy())\n",
    "        prediction = np.argmax(model(x[0]))\n",
    "        \n",
    "        heatmap = grad_cam(x[0], target, model, last_conv_layer_name)\n",
    "        \n",
    "        overlayed_image = overlay_heatmap(image, heatmap) \n",
    "    \n",
    "        print(\"CLASS :\", target)\n",
    "        print(\"PRED : \", prediction)\n",
    "        \n",
    "        plt.subplot(1,3,1) \n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\") \n",
    "        \n",
    "        plt.subplot(1,3,2) \n",
    "        plt.imshow(overlayed_image)\n",
    "        plt.axis(\"off\") \n",
    "    \n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(heatmap) \n",
    "        plt.axis(\"off\") \n",
    "    \n",
    "        plt.show()\n",
    "    \n",
    "        curr_plot = 1\n",
    "        for i in range(0,5):\n",
    "            if i != target:\n",
    "                plt.subplot(1,4,curr_plot) \n",
    "                heatmap = grad_cam(x[0], i, model, last_conv_layer_name)\n",
    "                plt.imshow(heatmap) \n",
    "                plt.axis(\"off\") \n",
    "                plt.title(str(i))\n",
    "                curr_plot += 1\n",
    "                \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9069f-1c24-4e2b-afef-c3701f9d2a6f",
   "metadata": {},
   "source": [
    "#### Resnet18-2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51750a12-425d-483a-923c-de5cec51e508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(224, 224, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_resnet18_2 \u001b[38;5;241m=\u001b[39m load_model(models_trained_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet18_2.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgenerate_grad_cam_plots\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_resnet18_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconv2d_19\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mgenerate_grad_cam_plots\u001b[0;34m(model, last_conv_layer_name, data, num_images)\u001b[0m\n\u001b[1;32m      4\u001b[0m image \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m] \n\u001b[1;32m      5\u001b[0m target \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(x[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m----> 6\u001b[0m prediction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m grad_cam(x[\u001b[38;5;241m0\u001b[39m], target, model, last_conv_layer_name)\n\u001b[1;32m     10\u001b[0m overlayed_image \u001b[38;5;241m=\u001b[39m overlay_heatmap(image, heatmap) \n",
      "File \u001b[0;32m~/tf_gpu/tf218/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/tf_gpu/tf218/lib/python3.12/site-packages/keras/src/layers/input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"functional\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(224, 224, 3)"
     ]
    }
   ],
   "source": [
    "model_resnet18_2 = load_model(models_trained_path / \"resnet18_2.keras\")\n",
    "generate_grad_cam_plots(model_resnet18_2, \"conv2d_19\", dataset[\"test\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
